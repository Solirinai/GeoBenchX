{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f91a78e",
   "metadata": {},
   "source": [
    "### This notebook contains code to:\n",
    "1. Generate candidate solutions for a set of tasks\n",
    "2. Evaluate set of candidate solutions in one file or a set of files\n",
    "3. Get evaluation stats by labels, by usage of specific tools (functions), the share of the match score and CI (Ward formula)\n",
    "\n",
    "Note that the reference solutions (groud truth solutions) need to be in the set you send to the evaluation. \n",
    "\n",
    "**Task_editor GUI** allow to edit the task text, generate draft ground ruth solution for the task, manually correct it and leave comments for evaluator on which steps are optional, what arguments are acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d04808-9c73-4fa9-8aaf-c13cebc76b14",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b9822-8b42-42c6-93f5-e7da4bccca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19c5d8f-fc6a-411d-bce2-b0c054c66f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99ae8a5-f37b-49c5-9b2d-43aff9de918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import geobenchx.utils\n",
    "importlib.reload(geobenchx.utils)\n",
    "from geobenchx.utils import generate_timestamp_id, get_dataframe_info, get_solution_code\n",
    "\n",
    "import geobenchx.dataclasses\n",
    "importlib.reload(geobenchx.dataclasses)\n",
    "from geobenchx.dataclasses import TaskSet\n",
    "\n",
    "\n",
    "import geobenchx.agent\n",
    "importlib.reload(geobenchx.agent)\n",
    "from geobenchx.agent import execute_task\n",
    "\n",
    "import geobenchx.evaluation\n",
    "importlib.reload(geobenchx.evaluation)\n",
    "from geobenchx.evaluation import score_solutions_set, generate_eval_stats, get_eval_stats_by_subsets\n",
    "\n",
    "from geobenchx.constants import DATA_FOLDER, RESULTS_FOLDER, MODEL_CLAUDE, MODEL_GEMINI, MODEL_GPT_41, MODEL_GEMINI_ADV, MODEL_CLAUDE_mini, MODEL_O3, MODEL_O4, MODEL_GPT_mini, MODEL_CLAUDE_ADV4\n",
    "\n",
    "import geobenchx.generate_solutions\n",
    "importlib.reload(geobenchx.generate_solutions)\n",
    "from geobenchx.generate_solutions import generate_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54d16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL_GEMINI\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc17e8-3230-4588-adbe-96ce021d50d6",
   "metadata": {},
   "source": [
    "## Generate solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4ce2bf-0a74-4ead-8306-748105b1dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select file with the task set to solve\n",
    "\n",
    "tasks_filename = r\"tasks_and_reference_solutions.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e978e317-c2fc-4f8c-bc68-1a0bd65c0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading tasks from that file\n",
    "\n",
    "tasks = TaskSet.read_from_file(tasks_filename, DATA_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ff319f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6adb55f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<TaskLabels.CONTROL: 'Control question'>: 3,\n",
       " <TaskLabels.SPATIAL_OPERATIONS: 'Spatial operations'>: 53,\n",
       " <TaskLabels.TASK_SET_03: 'Task Set 03'>: 53,\n",
       " <TaskLabels.HEATMAPS_CONTOUR_LINES: 'Heatmaps, Contour Lines'>: 54,\n",
       " <TaskLabels.TASK_SET_04: 'Task Set 04'>: 54,\n",
       " <TaskLabels.VAGUE: 'Vague'>: 4,\n",
       " <TaskLabels.MERGE_VISUALIZE: 'Merge, Visualize'>: 36,\n",
       " <TaskLabels.TASK_SET_01: 'Task Set 01'>: 36,\n",
       " <TaskLabels.PROCESS_MERGE_VISUALIZE: 'Process, Merge, Visualize'>: 56,\n",
       " <TaskLabels.TASK_SET_02: 'Task Set 02'>: 56}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the set composition\n",
    "tasks.get_labels_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4453520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a standartized name. Each generation will overwrite the candidate solutions and metadata, so if you have 1 output file name, it will be overwritten each time you run the code\n",
    "\n",
    "output_tasks_filename = f'generated_solutions_{model}_temp_{temperature}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d975ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generated_solutions_gemini-2.0-flash-001_temp_0.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tasks_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49257510-ce68-4f35-9a16-243592e5d668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating candidate solutions. Double check your output file name above\n",
    "\n",
    "tasks_solved, _, _ = generate_solutions(tasks, model = model, temperature=temperature, output_filename=output_tasks_filename, max_steps=25, skip_solved=False, capture_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43d5a4-da5c-4b44-9b86-7c0349e6f9e5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Evaluate solutions (single file, single LLM) and get evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285477cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the file with tasks and solutions to score the candidate solutions\n",
    "\n",
    "taskset_to_evaluate = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score solutions in the selected file, reads tasks inside the function\n",
    "\n",
    "score_solutions_set(tasks_filename=taskset_to_evaluate, folder=RESULTS_FOLDER, model=model, temperature=temperature, skip_scored= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa366d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation stats for all tasks - works on the task set, so read the task file first\n",
    "\n",
    "taskset_to_evaluate = TaskSet.read_from_file(taskset_to_evaluate, RESULTS_FOLDER)\n",
    "\n",
    "eval_stats = generate_eval_stats(taskset_to_evaluate)\n",
    "eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486010ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluations stat for solvable tasks and unsolvable task (reject_task in reference solutions)\n",
    "# You can use this function to get stats by task label if you added labels while creating tasks\n",
    "\n",
    "get_eval_stats_by_subsets(taskset_to_evaluate, RESULTS_FOLDER, functions_names=['reject_task'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bea2d6",
   "metadata": {},
   "source": [
    "## Batch final (evaluation of multiple files and by multiple LLMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_evaluate = [\n",
    "# input names of files as a Python list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a69b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    MODEL_CLAUDE,\n",
    "    MODEL_GPT_41,\n",
    "    MODEL_GEMINI_ADV\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in files_to_evaluate:\n",
    "    for model in models:\n",
    "        tasks = TaskSet.read_from_file(filename, RESULTS_FOLDER)\n",
    "        new_filename = 'eval_by_' + model + '_' + filename\n",
    "        tasks = TaskSet.save_to_file(tasks, new_filename, RESULTS_FOLDER)\n",
    "        score_solutions_set(tasks_filename=new_filename, folder=RESULTS_FOLDER, model=model, temperature=0, skip_scored=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c151e",
   "metadata": {},
   "source": [
    "## Calculating final stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c01a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "# files with scored solutions for evaluation as a Python list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "410d8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarking_result_filename = os.path.join(RESULTS_FOLDER, 'benchmark_results.json')\n",
    "with open(benchmarking_result_filename, 'r') as f:\n",
    "    benchmarking_res_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulting stats for all tasks\n",
    "\n",
    "for filename in filenames:\n",
    "    tasks = TaskSet.read_from_file(filename, RESULTS_FOLDER)\n",
    "    bd = generate_eval_stats(tasks)\n",
    "    benchmarking_res_dict[filename] = bd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d73a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulting stats by label\n",
    "\n",
    "for filename in filenames:\n",
    "    bd = get_eval_stats_by_subsets(filename, RESULTS_FOLDER, labels=['Task Set 01', 'Task Set 02', 'Task Set 03', 'Task Set 04', 'Control question'])\n",
    "    benchmarking_res_dict[filename] = bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulting stats by solvable and not\n",
    "\n",
    "for filename in filenames:\n",
    "    bd = get_eval_stats_by_subsets(filename, RESULTS_FOLDER, functions_names=['reject_task'])\n",
    "    benchmarking_res_dict[filename] = bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results\n",
    "\n",
    "with open(benchmarking_result_filename, 'w') as f:\n",
    "    json.dump(benchmarking_res_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
